{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute OFI Metrics:\n",
    "- Derive multi-level OFI metrics (up to 5 levels) for each stock in the dataset.\n",
    "- Integrate these multi-level OFIs into a single metric using Principal Component Analysis (PCA) or another dimensionality reduction method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_paths = sorted(glob.glob('../data/*.csv.zst'))  # Sort files by filename\n",
    "# data_list = [pd.read_csv(file_paths[i]) for i in range(5)]\n",
    "# data = pd.concat(data_list, ignore_index=True)\n",
    "data = pd.read_csv(\"../data/raw_data/dbeq-basic-20241216.mbp-10.csv.zst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_minute_level(data):\n",
    "    \"\"\"\n",
    "    Process data at the minute level by keeping the last state within each minute.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): DataFrame containing order book data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Minute-level processed data.\n",
    "    \"\"\"\n",
    "    data.loc[:, 'timestamp'] = pd.to_datetime(data.loc[:,'ts_recv']).dt.floor('T')\n",
    "    # Use groupby and last directly without sorting\n",
    "    minute_level_data = data.groupby(['symbol', 'timestamp']).last().reset_index()\n",
    "    minute_level_data.fillna(0, inplace=True)\n",
    "    return minute_level_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_columns = [\n",
    "    'ts_recv',    'action',\n",
    "    'side',\n",
    "    'price',\n",
    "    'size',\n",
    "    'sequence',\n",
    "    'bid_px_00',\n",
    "    'ask_px_00',\n",
    "    'bid_sz_00',\n",
    "    'ask_sz_00',\n",
    "    'bid_ct_00',\n",
    "    'ask_ct_00',\n",
    "    'bid_px_01',\n",
    "    'ask_px_01',\n",
    "    'bid_sz_01',\n",
    "    'ask_sz_01',\n",
    "    'bid_ct_01',\n",
    "    'ask_ct_01',\n",
    "    'bid_px_02',\n",
    "    'ask_px_02',\n",
    "    'bid_sz_02',\n",
    "    'ask_sz_02',\n",
    "    'bid_ct_02',\n",
    "    'ask_ct_02',\n",
    "    'bid_px_03',\n",
    "    'ask_px_03',\n",
    "    'bid_sz_03',\n",
    "    'ask_sz_03',\n",
    "    'bid_ct_03',\n",
    "    'ask_ct_03',\n",
    "    'bid_px_04',\n",
    "    'ask_px_04',\n",
    "    'bid_sz_04',\n",
    "    'ask_sz_04',\n",
    "    'bid_ct_04',\n",
    "    'ask_ct_04',\n",
    "    'symbol',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yj/jjmvh7dd6sx95p4gzltx4j2r0000gn/T/ipykernel_16293/1560544773.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:, 'timestamp'] = pd.to_datetime(data.loc[:,'ts_recv']).dt.floor('T')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>ts_recv</th>\n",
       "      <th>action</th>\n",
       "      <th>side</th>\n",
       "      <th>price</th>\n",
       "      <th>size</th>\n",
       "      <th>sequence</th>\n",
       "      <th>bid_px_00</th>\n",
       "      <th>ask_px_00</th>\n",
       "      <th>bid_sz_00</th>\n",
       "      <th>ask_sz_00</th>\n",
       "      <th>bid_ct_00</th>\n",
       "      <th>ask_ct_00</th>\n",
       "      <th>bid_px_01</th>\n",
       "      <th>ask_px_01</th>\n",
       "      <th>bid_sz_01</th>\n",
       "      <th>ask_sz_01</th>\n",
       "      <th>bid_ct_01</th>\n",
       "      <th>ask_ct_01</th>\n",
       "      <th>bid_px_02</th>\n",
       "      <th>ask_px_02</th>\n",
       "      <th>bid_sz_02</th>\n",
       "      <th>ask_sz_02</th>\n",
       "      <th>bid_ct_02</th>\n",
       "      <th>ask_ct_02</th>\n",
       "      <th>bid_px_03</th>\n",
       "      <th>ask_px_03</th>\n",
       "      <th>bid_sz_03</th>\n",
       "      <th>ask_sz_03</th>\n",
       "      <th>bid_ct_03</th>\n",
       "      <th>ask_ct_03</th>\n",
       "      <th>bid_px_04</th>\n",
       "      <th>ask_px_04</th>\n",
       "      <th>bid_sz_04</th>\n",
       "      <th>ask_sz_04</th>\n",
       "      <th>bid_ct_04</th>\n",
       "      <th>ask_ct_04</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 07:03:00+00:00</td>\n",
       "      <td>2024-12-16T07:03:00.029417437Z</td>\n",
       "      <td>R</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7247</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 07:04:00+00:00</td>\n",
       "      <td>2024-12-16T07:04:00.028766595Z</td>\n",
       "      <td>R</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:00:00+00:00</td>\n",
       "      <td>2024-12-16T12:00:51.236171281Z</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>247.6</td>\n",
       "      <td>2000</td>\n",
       "      <td>15333</td>\n",
       "      <td>247.6</td>\n",
       "      <td>249.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:01:00+00:00</td>\n",
       "      <td>2024-12-16T12:01:38.629222193Z</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>249.6</td>\n",
       "      <td>2000</td>\n",
       "      <td>16126</td>\n",
       "      <td>247.6</td>\n",
       "      <td>249.6</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247.2</td>\n",
       "      <td>250.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:02:00+00:00</td>\n",
       "      <td>2024-12-16T12:02:57.178549321Z</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>249.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>17143</td>\n",
       "      <td>247.6</td>\n",
       "      <td>249.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247.4</td>\n",
       "      <td>249.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:03:00+00:00</td>\n",
       "      <td>2024-12-16T12:03:21.144528379Z</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>247.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>17349</td>\n",
       "      <td>247.4</td>\n",
       "      <td>249.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:04:00+00:00</td>\n",
       "      <td>2024-12-16T12:04:46.939932329Z</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>247.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>18103</td>\n",
       "      <td>247.6</td>\n",
       "      <td>249.4</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:05:00+00:00</td>\n",
       "      <td>2024-12-16T12:05:30.815491776Z</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>249.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>18535</td>\n",
       "      <td>247.6</td>\n",
       "      <td>249.6</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>249.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:06:00+00:00</td>\n",
       "      <td>2024-12-16T12:06:16.522543203Z</td>\n",
       "      <td>C</td>\n",
       "      <td>B</td>\n",
       "      <td>247.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>19155</td>\n",
       "      <td>247.6</td>\n",
       "      <td>249.6</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2024-12-16 12:07:00+00:00</td>\n",
       "      <td>2024-12-16T12:07:41.238933899Z</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>249.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>19897</td>\n",
       "      <td>247.4</td>\n",
       "      <td>249.8</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>247.0</td>\n",
       "      <td>249.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  symbol                 timestamp                         ts_recv action  \\\n",
       "0   AAPL 2024-12-16 07:03:00+00:00  2024-12-16T07:03:00.029417437Z      R   \n",
       "1   AAPL 2024-12-16 07:04:00+00:00  2024-12-16T07:04:00.028766595Z      R   \n",
       "2   AAPL 2024-12-16 12:00:00+00:00  2024-12-16T12:00:51.236171281Z      A   \n",
       "3   AAPL 2024-12-16 12:01:00+00:00  2024-12-16T12:01:38.629222193Z      A   \n",
       "4   AAPL 2024-12-16 12:02:00+00:00  2024-12-16T12:02:57.178549321Z      A   \n",
       "5   AAPL 2024-12-16 12:03:00+00:00  2024-12-16T12:03:21.144528379Z      A   \n",
       "6   AAPL 2024-12-16 12:04:00+00:00  2024-12-16T12:04:46.939932329Z      C   \n",
       "7   AAPL 2024-12-16 12:05:00+00:00  2024-12-16T12:05:30.815491776Z      C   \n",
       "8   AAPL 2024-12-16 12:06:00+00:00  2024-12-16T12:06:16.522543203Z      C   \n",
       "9   AAPL 2024-12-16 12:07:00+00:00  2024-12-16T12:07:41.238933899Z      A   \n",
       "\n",
       "  side  price  size  sequence  bid_px_00  ask_px_00  bid_sz_00  ask_sz_00  \\\n",
       "0    N    0.0     0      7247        0.0        0.0          0          0   \n",
       "1    N    0.0     0      7114        0.0        0.0          0          0   \n",
       "2    B  247.6  2000     15333      247.6      249.4       2000       2000   \n",
       "3    A  249.6  2000     16126      247.6      249.6       2000       2000   \n",
       "4    A  249.4  2000     17143      247.6      249.4       2000       2000   \n",
       "5    B  247.4  2000     17349      247.4      249.4       2000       2000   \n",
       "6    B  247.4  2000     18103      247.6      249.4       2000       2000   \n",
       "7    A  249.8  2000     18535      247.6      249.6       2000       2000   \n",
       "8    B  247.2  2000     19155      247.6      249.6       2000       2000   \n",
       "9    A  249.8  2000     19897      247.4      249.8       2000       2000   \n",
       "\n",
       "   bid_ct_00  ask_ct_00  bid_px_01  ask_px_01  bid_sz_01  ask_sz_01  \\\n",
       "0          0          0        0.0        0.0          0          0   \n",
       "1          0          0        0.0        0.0          0          0   \n",
       "2          1          1        0.0      249.6          0          0   \n",
       "3          1          1      247.2      250.0          0          0   \n",
       "4          1          1      247.4      249.8          0          0   \n",
       "5          1          1        0.0        0.0          0          0   \n",
       "6          1          1      247.4        0.0          0          0   \n",
       "7          1          1        0.0      249.8          0          0   \n",
       "8          1          1      247.2        0.0          0          0   \n",
       "9          1          1      247.0      249.6          0          0   \n",
       "\n",
       "   bid_ct_01  ask_ct_01  bid_px_02  ask_px_02  bid_sz_02  ask_sz_02  \\\n",
       "0          0          0        0.0        0.0          0          0   \n",
       "1          0          0        0.0        0.0          0          0   \n",
       "2          0          0        0.0        0.0          0          0   \n",
       "3          0          0        0.0        0.0          0          0   \n",
       "4          0          0        0.0        0.0          0          0   \n",
       "5          0          0        0.0        0.0          0          0   \n",
       "6          0          0        0.0        0.0          0          0   \n",
       "7          0          0        0.0        0.0          0          0   \n",
       "8          0          0        0.0        0.0          0          0   \n",
       "9          0          0        0.0        0.0          0          0   \n",
       "\n",
       "   bid_ct_02  ask_ct_02  bid_px_03  ask_px_03  bid_sz_03  ask_sz_03  \\\n",
       "0          0          0        0.0        0.0          0          0   \n",
       "1          0          0        0.0        0.0          0          0   \n",
       "2          0          0        0.0        0.0          0          0   \n",
       "3          0          0        0.0        0.0          0          0   \n",
       "4          0          0        0.0        0.0          0          0   \n",
       "5          0          0        0.0        0.0          0          0   \n",
       "6          0          0        0.0        0.0          0          0   \n",
       "7          0          0        0.0        0.0          0          0   \n",
       "8          0          0        0.0        0.0          0          0   \n",
       "9          0          0        0.0        0.0          0          0   \n",
       "\n",
       "   bid_ct_03  ask_ct_03  bid_px_04  ask_px_04  bid_sz_04  ask_sz_04  \\\n",
       "0          0          0        0.0        0.0          0          0   \n",
       "1          0          0        0.0        0.0          0          0   \n",
       "2          0          0        0.0        0.0          0          0   \n",
       "3          0          0        0.0        0.0          0          0   \n",
       "4          0          0        0.0        0.0          0          0   \n",
       "5          0          0        0.0        0.0          0          0   \n",
       "6          0          0        0.0        0.0          0          0   \n",
       "7          0          0        0.0        0.0          0          0   \n",
       "8          0          0        0.0        0.0          0          0   \n",
       "9          0          0        0.0        0.0          0          0   \n",
       "\n",
       "   bid_ct_04  ask_ct_04  \n",
       "0          0          0  \n",
       "1          0          0  \n",
       "2          0          0  \n",
       "3          0          0  \n",
       "4          0          0  \n",
       "5          0          0  \n",
       "6          0          0  \n",
       "7          0          0  \n",
       "8          0          0  \n",
       "9          0          0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = data[required_columns]\n",
    "# df['timestamp'] = pd.to_datetime(data.loc[:, 'ts_recv']).dt.floor('T')\n",
    "minute_level_data = process_minute_level(data[required_columns])\n",
    "minute_level_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OFI metrics have been successfully computed and saved.\n"
     ]
    }
   ],
   "source": [
    "def compute_ofi(data, levels=5):\n",
    "    \"\"\"\n",
    "    Compute multi-level OFI metrics.\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Input data containing LOB states.\n",
    "        levels (int): Number of levels to compute OFI for.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing OFI metrics for each stock.\n",
    "    \"\"\"\n",
    "    ofi_results = []\n",
    "\n",
    "    for symbol, group in data.groupby('symbol'):\n",
    "        # Calculate mid-price\n",
    "        group['mid_price'] = (group['bid_px_00'] + group['ask_px_00']) / 2\n",
    "\n",
    "        # Filter out rows where mid_price is zero\n",
    "        group = group[group['mid_price'] > 0].reset_index(drop=True)\n",
    "\n",
    "        # Initialize dictionary for storing OFI and mid-price\n",
    "        symbol_ofi = {\n",
    "            'symbol': group['symbol'],\n",
    "            'timestamp': group['timestamp'],\n",
    "            'mid_price': group['mid_price']\n",
    "        }\n",
    "\n",
    "        for level in range(0, levels):\n",
    "            bid_price_col = f'bid_px_0{level}'\n",
    "            bid_size_col = f'bid_sz_0{level}'\n",
    "            ask_price_col = f'ask_px_0{level}'\n",
    "            ask_size_col = f'ask_sz_0{level}'\n",
    "\n",
    "            # Compute bid and ask OFIs\n",
    "            bid_ofi = np.where(\n",
    "                group[bid_price_col].diff() > 0, group[bid_size_col],\n",
    "                np.where(\n",
    "                    group[bid_price_col].diff() == 0,\n",
    "                    group[bid_size_col].diff(),\n",
    "                    -group[bid_size_col]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            ask_ofi = np.where(\n",
    "                group[ask_price_col].diff() > 0, -group[ask_size_col],\n",
    "                np.where(\n",
    "                    group[ask_price_col].diff() == 0,\n",
    "                    group[ask_size_col].diff(),\n",
    "                    group[ask_size_col]\n",
    "                )\n",
    "            )\n",
    "\n",
    "            level_ofi = bid_ofi - ask_ofi\n",
    "            symbol_ofi[f'ofi_level_{level}'] = level_ofi\n",
    "\n",
    "        # Convert to DataFrame and append to results\n",
    "        ofi_results.append(pd.DataFrame(symbol_ofi))\n",
    "\n",
    "    return pd.concat(ofi_results, ignore_index=True)\n",
    "\n",
    "ofi_metrics = compute_ofi(minute_level_data, levels=5)\n",
    "# Save the results\n",
    "ofi_metrics.to_csv('../data/ofi_metrics/20241216_ofi_metrics.csv', index=False)\n",
    "\n",
    "print(\"OFI metrics have been successfully computed and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_ofi_with_pca(daily_data, date, levels=5):\n",
    "    \"\"\"\n",
    "    Integrate multi-level OFIs into a single metric using PCA for each stock in a given trading day.\n",
    "\n",
    "    Args:\n",
    "        daily_data (pd.DataFrame): DataFrame containing the daily OFI metrics.\n",
    "        date (str): Trading day date as a string.\n",
    "        levels (int): Number of levels used in OFI computation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Updated DataFrame with integrated OFI and explained variance.\n",
    "        list: A list of dictionaries containing explained variance data for each stock on the given day.\n",
    "    \"\"\"\n",
    "    explained_variances = []\n",
    "\n",
    "    for symbol, group in daily_data.groupby('symbol'):\n",
    "        pca_columns = [f'ofi_level_{i}' for i in range(levels)]\n",
    "        original_indices = group.index  # Track original indices for correct assignment later\n",
    "\n",
    "        # Check if all columns are null or constant\n",
    "        if group[pca_columns].isnull().all().any() or group[pca_columns].nunique().max() == 1:\n",
    "            print(f\"Skipping PCA for symbol {symbol} on {date}: insufficient data.\")\n",
    "            daily_data.loc[original_indices, 'integrated_ofi'] = np.nan\n",
    "            explained_variances.append({\n",
    "                'date': date,\n",
    "                'symbol': symbol,\n",
    "                'explained_variance': np.nan\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # Perform PCA on multi-level OFI columns\n",
    "        pca = PCA(n_components=1)\n",
    "        group['integrated_ofi'] = pca.fit_transform(group[pca_columns])\n",
    "\n",
    "        # Update the original daily_data DataFrame with the calculated 'integrated_ofi'\n",
    "        daily_data.loc[original_indices, 'integrated_ofi'] = group['integrated_ofi']\n",
    "\n",
    "        # Collect explained variance\n",
    "        explained_variances.append({\n",
    "            'date': date,\n",
    "            'symbol': symbol,\n",
    "            'explained_variance': pca.explained_variance_ratio_[0]\n",
    "        })\n",
    "\n",
    "    return daily_data, explained_variances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"../data/ofi_metrics/*_ofi_metrics.csv\")\n",
    "all_explained_variances = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    daily_data = pd.read_csv(file_path)\n",
    "    daily_data, daily_explained_variances = integrate_ofi_with_pca(daily_data, date, levels=5)\n",
    "\n",
    "    # Save updated daily data with integrated OFI\n",
    "    daily_data.to_csv(file_path, index=False)\n",
    "\n",
    "    # Collect explained variance data\n",
    "    all_explained_variances.extend(daily_explained_variances)\n",
    "    \n",
    "explained_variance_df = pd.DataFrame(all_explained_variances)\n",
    "explained_variance_matrix = explained_variance_df.pivot(index='date', columns='symbol', values='explained_variance')\n",
    "explained_variance_matrix.to_csv('../data/ofi_metrics/explained_variance_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Cross-Impact:\n",
    "- Examine the contemporaneous cross-impact of OFI on short-term price changes\n",
    "across stocks.\n",
    "- Evaluate the predictive power of lagged cross-asset OFI on future price changes\n",
    "(e.g., 1-minute and 5-minute horizons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_contemporaneous_cross_impact(daily_data):\n",
    "    \"\"\"\n",
    "    Analyze contemporaneous cross-impact of OFI on short-term price changes across stocks.\n",
    "\n",
    "    Args:\n",
    "        daily_data (pd.DataFrame): DataFrame containing the daily OFI metrics.\n",
    "\n",
    "    Returns:\n",
    "        float: R^2 value of cross-impact analysis for the given day.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Prepare integrated OFI data for all stocks\n",
    "    integrated_ofi = daily_data.pivot(index='timestamp', columns='symbol', values='integrated_ofi')\n",
    "    mid_prices = daily_data.pivot(index='timestamp', columns='symbol', values='mid_price')\n",
    "    mid_price_changes = np.log(mid_prices).diff()[1:]\n",
    "\n",
    "    # Loop through each stock for individual models\n",
    "    for stock in mid_price_changes.columns:\n",
    "        Y = mid_price_changes[stock].dropna()\n",
    "        X = integrated_ofi.loc[Y.index].ffill().bfill()  # Match X and Y by index\n",
    "\n",
    "        if X.empty or Y.empty:\n",
    "            print(f\"Skipping stock {stock}: insufficient data for regression.\")\n",
    "            continue\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, Y)\n",
    "\n",
    "        r2 = r2_score(Y, model.predict(X))\n",
    "        coefficients = model.coef_\n",
    "        intercept = model.intercept_\n",
    "\n",
    "        result = {'stock': stock, 'r2': r2, 'intercept': intercept}\n",
    "        for idx, coeff in enumerate(coefficients):\n",
    "            result[f'coef_{integrated_ofi.columns[idx]}'] = coeff\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"../data/ofi_metrics/*_ofi_metrics.csv\")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    # print(date)\n",
    "    daily_data = pd.read_csv(file_path)\n",
    "    regression_results = analyze_contemporaneous_cross_impact(daily_data)\n",
    "    regression_results_df = pd.DataFrame(regression_results)\n",
    "    # Save updated daily data with integrated OFI\n",
    "    regression_results_df.to_csv(f\"../data/contemporaneous_regression_results/{date}_result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_predictive_power(daily_data, horizons=[1, 5]):\n",
    "    \"\"\"\n",
    "    Evaluate the predictive power of lagged cross-asset OFI on future price changes.\n",
    "\n",
    "    Args:\n",
    "        daily_data (pd.DataFrame): DataFrame containing the daily OFI metrics.\n",
    "        horizons (list): List of time horizons (in minutes) for predictive analysis.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame containing R^2 values, coefficients, and intercepts for each stock and horizon.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Prepare integrated OFI data for all stocks\n",
    "    integrated_ofi = daily_data.pivot(index='timestamp', columns='symbol', values='integrated_ofi')\n",
    "    mid_prices = daily_data.pivot(index='timestamp', columns='symbol', values='mid_price')\n",
    "\n",
    "    for horizon in horizons:\n",
    "        future_price_changes = np.log(mid_prices.shift(-horizon)) - np.log(mid_prices)\n",
    "\n",
    "        for stock in future_price_changes.columns:\n",
    "            Y = future_price_changes[stock].dropna()\n",
    "            X = integrated_ofi.loc[Y.index].ffill().bfill()  # Match X and Y by index\n",
    "\n",
    "            if X.empty or Y.empty:\n",
    "                print(f\"Skipping stock {stock} for horizon {horizon}: insufficient data for regression.\")\n",
    "                continue\n",
    "\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, Y)\n",
    "\n",
    "            r2 = r2_score(Y, model.predict(X))\n",
    "            coefficients = model.coef_\n",
    "            intercept = model.intercept_\n",
    "\n",
    "            result = {'stock': stock, 'horizon': horizon, 'r2': r2, 'intercept': intercept}\n",
    "            for idx, coeff in enumerate(coefficients):\n",
    "                result[f'coef_{integrated_ofi.columns[idx]}'] = coeff\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"../data/ofi_metrics/*_ofi_metrics.csv\")\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    # print(date)\n",
    "    daily_data = pd.read_csv(file_path)\n",
    "    regression_results = analyze_predictive_power(daily_data)\n",
    "    regression_results_df = pd.DataFrame(regression_results)\n",
    "    # Save updated daily data with integrated OFI\n",
    "    regression_results_df.to_csv(f\"../data/predictive_regression_results/{date}_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantify Results:\n",
    "- Use regression models to assess the explanatory power of contemporaneous\n",
    "OFI and predictive power of lagged OFI.\n",
    "- Compare self-impact (within the same stock) vs. cross-impact (between stocks)\n",
    "in your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_explanatory_power(cross_impact_results):\n",
    "    \"\"\"\n",
    "    Evaluate explanatory power of contemporaneous OFI models.\n",
    "\n",
    "    Args:\n",
    "        cross_impact_results (pd.DataFrame): DataFrame containing R^2 and coefficients from contemporaneous models.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Aggregated metrics for explanatory power.\n",
    "    \"\"\"\n",
    "    grouped = cross_impact_results.groupby('stock').mean()\n",
    "\n",
    "    # Prepare a readable matrix output\n",
    "    summary_matrix = grouped.filter(like='coef_').copy()\n",
    "    summary_matrix['avg_r2'] = grouped['r2']\n",
    "\n",
    "    return summary_matrix.reset_index()\n",
    "\n",
    "\n",
    "def evaluate_predictive_power(predictive_power_results):\n",
    "    \"\"\"\n",
    "    Evaluate predictive power of lagged OFI models.\n",
    "\n",
    "    Args:\n",
    "        predictive_power_results (pd.DataFrame): DataFrame containing R^2 and coefficients from predictive models.\n",
    "        output_excel_path (str): Path to save the Excel file with results for each horizon.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    summary_matrix_dictionary = {}\n",
    "    for horizon in predictive_power_results['horizon'].unique():\n",
    "        subset = predictive_power_results[predictive_power_results['horizon'] == horizon]\n",
    "        explanatory_summary = evaluate_explanatory_power(subset)\n",
    "        summary_matrix_dictionary[horizon] = explanatory_summary\n",
    "\n",
    "    return summary_matrix_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"../data/contemporaneous_regression_results/*_result.csv\")\n",
    "cross_impact_results = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    daily_result = pd.read_csv(file_path)\n",
    "    cross_impact_results.append(daily_result)\n",
    "cross_impact_combined = pd.concat(cross_impact_results, ignore_index = True)\n",
    "cross_impact_combined = cross_impact_combined.loc[:, ~cross_impact_combined.columns.str.contains('^Unnamed')]\n",
    "explanatory_summary = evaluate_explanatory_power(cross_impact_combined)\n",
    "explanatory_summary.to_csv('../data/contemporaneous_regression_results/explanatory_power_summary.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = glob.glob(\"../data/predictive_regression_results/*_result.csv\")\n",
    "predictive_regression_results = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    daily_result = pd.read_csv(file_path)\n",
    "    predictive_regression_results.append(daily_result)\n",
    "\n",
    "predictive_power_combined = pd.concat(predictive_regression_results, ignore_index = True)\n",
    "predictive_power_combined = predictive_power_combined.loc[:, ~predictive_power_combined.columns.str.contains('^Unnamed')]\n",
    "predictive_summary_dict = evaluate_predictive_power(predictive_power_combined)\n",
    "\n",
    "output_excel_path = \"../data/predictive_regression_results/predictive_power_summary.xlsx\"\n",
    "os.makedirs(os.path.dirname(output_excel_path), exist_ok=True)\n",
    "\n",
    "with pd.ExcelWriter(output_excel_path, engine='openpyxl') as writer:\n",
    "        for horizon in predictive_summary_dict:\n",
    "            subset = predictive_summary_dict[horizon]\n",
    "            subset.to_excel(writer, sheet_name=f'Horizon_{horizon}', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "- Create visualizations (e.g., heatmaps, scatter plots) to illustrate cross-impact relationships and OFI trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_heatmap(data, title=\"heatmap\", output_dir=\"../results/\"):\n",
    "    \"\"\"\n",
    "    Generate a heatmap image based on the given dataframe.\n",
    "\n",
    "    Args:\n",
    "        data (pd.DataFrame): Summary DataFrame.\n",
    "        title (str): Title for the heatmap and filename.\n",
    "        output_dir (str): Directory to save the heatmap.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Prepare output path\n",
    "    output_path = os.path.join(output_dir, f\"{title} Heatmap.png\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Prepare data for heatmap\n",
    "    heatmap_data = data.set_index('stock').filter(like='coef_').fillna(0)\n",
    "    \n",
    "    # Format values for annotations\n",
    "    annotation_data = heatmap_data.applymap(lambda x: f\"{x:.2e}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(\n",
    "        heatmap_data,\n",
    "        annot=annotation_data,\n",
    "        fmt=\"\",\n",
    "        cmap=\"coolwarm\",\n",
    "        cbar_kws={\"format\": \"%.1e\"},\n",
    "        linewidths=0.5\n",
    "    )\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha=\"right\")  # Rotate x-axis labels for better readability\n",
    "    plt.yticks(rotation=0)  # Keep y-axis labels horizontal\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Heatmap saved to {output_path}.\")\n",
    "\n",
    "\n",
    "def visualize_scatter_plot(data, x_variable, y_variable, title = \"scatter_plot\", output_dir=\"../results/\"):\n",
    "    \"\"\"\n",
    "    Generate scatter plots for predictive power results with X as time horizon, Y as R^2, and legend as different stocks.\n",
    "\n",
    "    Args:\n",
    "        predictive_power_results (pd.DataFrame): DataFrame containing predictive power results.\n",
    "        output_dir (str): Directory to save the scatter plots.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Prepare output path\n",
    "    output_path = os.path.join(output_dir, f\"{title} scatterplot.png\")\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for stock, group in data.groupby('stock'):\n",
    "        plt.plot(group[x_variable], group[y_variable], marker='o', label=stock)\n",
    "    plt.xlabel(f\"{x_variable}\")\n",
    "    plt.ylabel(f\"{y_variable}\")\n",
    "    plt.title(f\"{title}\")\n",
    "    plt.legend(title='Stock')\n",
    "    plt.grid(True)\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "    print(f\"Scatter plot saved to {output_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yj/jjmvh7dd6sx95p4gzltx4j2r0000gn/T/ipykernel_11628/3773728321.py:21: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  annotation_data = heatmap_data.applymap(lambda x: f\"{x:.2e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heatmap saved to ../results/predictive_power_horizon_5.png.\n"
     ]
    }
   ],
   "source": [
    "summary_df = pd.read_excel(\"../data/predictive_regression_results/predictive_power_summary.xlsx\", sheet_name = 1)\n",
    "visualize_heatmap(summary_df, title = \"predictive_power_horizon_5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to ../results/scatter_plot scatterplot.png.\n"
     ]
    }
   ],
   "source": [
    "file_paths = sorted(glob.glob(\"../data/contemporaneous_regression_results/*_result.csv\"))\n",
    "contemporaneous_regression_results = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    daily_result = pd.read_csv(file_path)\n",
    "    daily_result['date'] = date  # Add trading day column\n",
    "    contemporaneous_regression_results.append(daily_result)\n",
    "\n",
    "contemporaneous_impact_combined = pd.concat(contemporaneous_regression_results, ignore_index=True)\n",
    "contemporaneous_impact_combined = contemporaneous_impact_combined.loc[:, ~contemporaneous_impact_combined.columns.str.contains('^Unnamed')]\n",
    "\n",
    "visualize_scatter_plot(contemporaneous_impact_combined, x_variable=\"date\", y_variable=\"r2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scatter plot saved to ../results/predictive_power_horizon_1 scatterplot.png.\n",
      "Scatter plot saved to ../results/predictive_power_horizon_5 scatterplot.png.\n"
     ]
    }
   ],
   "source": [
    "file_paths = sorted(glob.glob(\"../data/predictive_regression_results/*_result.csv\"))\n",
    "contemporaneous_regression_results = []\n",
    "\n",
    "for file_path in file_paths:\n",
    "    date = file_path.split('/')[-1].split('_')[0]  # Extract date from filename\n",
    "    daily_result = pd.read_csv(file_path)\n",
    "    daily_result['date'] = date  # Add trading day column\n",
    "    contemporaneous_regression_results.append(daily_result)\n",
    "\n",
    "\n",
    "predictive_power_combined = pd.concat(contemporaneous_regression_results, ignore_index=True)\n",
    "predictive_power_combined  = predictive_power_combined .loc[:, ~predictive_power_combined .columns.str.contains('^Unnamed')]\n",
    "\n",
    "for horizon in [1, 5]:\n",
    "    subset = predictive_power_combined[predictive_power_combined['horizon'] == horizon]\n",
    "    visualize_scatter_plot(subset, title=f\"predictive_power_horizon_{horizon}\", x_variable=\"date\", y_variable=\"r2\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
